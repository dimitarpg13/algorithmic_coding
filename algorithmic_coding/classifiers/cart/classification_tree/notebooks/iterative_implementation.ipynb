{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iterative Classification Tree (CART) - Testing & Visualization\n",
        "\n",
        "This notebook demonstrates and tests the **iterative** implementation of a Classification and Regression Tree (CART) for classification tasks. Unlike the recursive version, this implementation uses explicit stack management.\n",
        "\n",
        "## Contents\n",
        "1. **Setup & Imports**\n",
        "2. **Basic Classification Tests**\n",
        "3. **Decision Boundary Visualization**\n",
        "4. **XOR Problem (Non-Linear)**\n",
        "5. **Multiclass Classification**\n",
        "6. **Hyperparameter Tuning Analysis**\n",
        "7. **Overfitting Demonstration**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "repo_url = \"https://github.com/dimitarpg13/algorithmic_coding.git\"\n",
        "target_dir = \"algorithmic_coding\"\n",
        "\n",
        "if not os.path.exists(target_dir):\n",
        "    !git clone {repo_url}\n",
        "else:\n",
        "    print(f\"Directory '{target_dir}' already exists. Skipping clone.\")\n",
        "\n",
        "\n",
        "%cd algorithmic_coding/\n",
        "\n",
        "repo_dir = os.getcwd()\n",
        "\n",
        "# Add the directory to the Python path\n",
        "if repo_dir not in sys.path:\n",
        "    sys.path.append(repo_dir)\n",
        "\n",
        "from classifiers.cart.classification_tree.recursive_implementation import ClassificationTree\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "# Color palettes - vibrant colors for clear visualization\n",
        "COLORS_LIGHT = ['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA']\n",
        "COLORS_DARK = ['#E74C3C', '#27AE60', '#3498DB', '#F39C12']\n",
        "\n",
        "print(\"✓ Setup complete!\")\n",
        "print(f\"Using ClassificationTree from iterative_implementation module\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Basic Classification Tests\n",
        "\n",
        "Let's verify the iterative classifier works correctly on simple datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Simple linearly separable data\n",
        "print(\"Test 1: Linearly Separable Binary Classification\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "X_simple = np.array([\n",
        "    [1, 2], [2, 3], [3, 1], [4, 2],  # Class 0 (bottom-left cluster)\n",
        "    [6, 7], [7, 8], [8, 6], [9, 7]   # Class 1 (top-right cluster)\n",
        "])\n",
        "y_simple = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
        "\n",
        "clf = ClassificationTree(max_depth=5)\n",
        "clf.fit(X_simple, y_simple)\n",
        "\n",
        "print(f\"Training accuracy: {clf.score(X_simple, y_simple):.2%}\")\n",
        "print(f\"Tree depth: {clf.get_depth()}\")\n",
        "print(f\"Number of leaves: {clf.get_n_leaves()}\")\n",
        "print(f\"\\nPredictions: {clf.predict(X_simple)}\")\n",
        "print(f\"True labels: {y_simple}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Probability predictions\n",
        "print(\"Test 2: Probability Predictions\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "proba = clf.predict_proba(X_simple)\n",
        "print(\"Class probabilities:\")\n",
        "print(f\"  Sample 0 (class 0): P(0)={proba[0, 0]:.2f}, P(1)={proba[0, 1]:.2f}\")\n",
        "print(f\"  Sample 4 (class 1): P(0)={proba[4, 0]:.2f}, P(1)={proba[4, 1]:.2f}\")\n",
        "\n",
        "# Verify probabilities sum to 1\n",
        "print(f\"\\nAll probabilities sum to 1: {np.allclose(proba.sum(axis=1), 1)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Decision Boundary Visualization\n",
        "\n",
        "Visualize how the tree partitions the feature space with axis-aligned splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_decision_boundary(clf, X, y, title=\"Decision Boundary\", ax=None, show_misclassified=False):\n",
        "    \"\"\"\n",
        "    Plot the decision boundary of a classifier along with data points.\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    \n",
        "    # Create mesh grid\n",
        "    h = 0.02  # Step size\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Predict on mesh\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Get number of classes\n",
        "    n_classes = len(np.unique(y))\n",
        "    cmap_light = ListedColormap(COLORS_LIGHT[:n_classes])\n",
        "    cmap_dark = ListedColormap(COLORS_DARK[:n_classes])\n",
        "    \n",
        "    # Plot decision regions\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\n",
        "    ax.contour(xx, yy, Z, colors='gray', linewidths=0.5, alpha=0.5)\n",
        "    \n",
        "    # Plot data points\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_dark, \n",
        "                         edgecolors='white', s=100, linewidths=1.5)\n",
        "    \n",
        "    # Mark misclassified if requested\n",
        "    if show_misclassified:\n",
        "        y_pred = clf.predict(X)\n",
        "        misclassified = y_pred != y\n",
        "        if misclassified.any():\n",
        "            ax.scatter(X[misclassified, 0], X[misclassified, 1],\n",
        "                      facecolors='none', edgecolors='black', s=200, linewidths=2,\n",
        "                      label=f'Misclassified ({misclassified.sum()})')\n",
        "            ax.legend(loc='upper left')\n",
        "    \n",
        "    ax.set_xlabel('Feature 1', fontsize=12)\n",
        "    ax.set_ylabel('Feature 2', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    \n",
        "    return ax\n",
        "\n",
        "# Visualize simple classification\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "plot_decision_boundary(clf, X_simple, y_simple, \n",
        "                      title=\"Decision Boundary - Simple Binary Classification\", ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Gaussian Clusters - Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Gaussian clusters\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 100\n",
        "mean_0 = [2, 2]\n",
        "mean_1 = [6, 6]\n",
        "cov = [[1, 0.5], [0.5, 1]]\n",
        "\n",
        "X_0 = np.random.multivariate_normal(mean_0, cov, n_samples)\n",
        "X_1 = np.random.multivariate_normal(mean_1, cov, n_samples)\n",
        "\n",
        "X_gauss = np.vstack([X_0, X_1])\n",
        "y_gauss = np.array([0] * n_samples + [1] * n_samples)\n",
        "\n",
        "# Shuffle data\n",
        "shuffle_idx = np.random.permutation(len(y_gauss))\n",
        "X_gauss = X_gauss[shuffle_idx]\n",
        "y_gauss = y_gauss[shuffle_idx]\n",
        "\n",
        "# Split train/test (80/20)\n",
        "split = int(0.8 * len(y_gauss))\n",
        "X_train, X_test = X_gauss[:split], X_gauss[split:]\n",
        "y_train, y_test = y_gauss[:split], y_gauss[split:]\n",
        "\n",
        "# Fit classifier\n",
        "clf_gauss = ClassificationTree(max_depth=5)\n",
        "clf_gauss.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gaussian Clusters Dataset\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Training accuracy: {clf_gauss.score(X_train, y_train):.2%}\")\n",
        "print(f\"Test accuracy: {clf_gauss.score(X_test, y_test):.2%}\")\n",
        "print(f\"Tree depth: {clf_gauss.get_depth()}\")\n",
        "print(f\"Number of leaves: {clf_gauss.get_n_leaves()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Gaussian clusters - Training vs Test\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Training data\n",
        "plot_decision_boundary(clf_gauss, X_train, y_train, \n",
        "                      title=\"Training Data & Decision Boundary\", ax=axes[0])\n",
        "\n",
        "# Test data with misclassified points highlighted\n",
        "plot_decision_boundary(clf_gauss, X_test, y_test, \n",
        "                      title=\"Test Data & Decision Boundary\", ax=axes[1],\n",
        "                      show_misclassified=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. XOR Problem (Non-Linear Decision Boundary)\n",
        "\n",
        "The XOR problem requires multiple splits and demonstrates that decision trees can learn non-linear boundaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate XOR pattern\n",
        "np.random.seed(42)\n",
        "n_per_corner = 40\n",
        "noise = 0.3\n",
        "\n",
        "# Four corners: (0,0)->0, (0,3)->1, (3,0)->1, (3,3)->0\n",
        "corners = [(0, 0, 0), (0, 3, 1), (3, 0, 1), (3, 3, 0)]\n",
        "X_xor_list = []\n",
        "y_xor_list = []\n",
        "\n",
        "for cx, cy, label in corners:\n",
        "    X_corner = np.random.randn(n_per_corner, 2) * noise + np.array([cx, cy])\n",
        "    X_xor_list.append(X_corner)\n",
        "    y_xor_list.extend([label] * n_per_corner)\n",
        "\n",
        "X_xor = np.vstack(X_xor_list)\n",
        "y_xor = np.array(y_xor_list)\n",
        "\n",
        "# Fit classifier\n",
        "clf_xor = ClassificationTree(max_depth=6)\n",
        "clf_xor.fit(X_xor, y_xor)\n",
        "\n",
        "print(\"XOR Problem Dataset\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Total samples: {len(X_xor)}\")\n",
        "print(f\"Training accuracy: {clf_xor.score(X_xor, y_xor):.2%}\")\n",
        "print(f\"Tree depth: {clf_xor.get_depth()}\")\n",
        "print(f\"Number of leaves: {clf_xor.get_n_leaves()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize XOR decision boundary\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "plot_decision_boundary(clf_xor, X_xor, y_xor, \n",
        "                      title=\"XOR Problem - Decision Boundary (Requires Multiple Splits)\", ax=ax)\n",
        "\n",
        "# Add corner labels\n",
        "for cx, cy, label in corners:\n",
        "    ax.annotate(f'Class {label}', xy=(cx, cy), fontsize=12, fontweight='bold',\n",
        "               ha='center', va='center', color='white',\n",
        "               bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Multiclass Classification\n",
        "\n",
        "Testing the classifier on a 3-class problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate 3-class dataset\n",
        "np.random.seed(123)\n",
        "\n",
        "n_samples = 80\n",
        "centers = [[1, 1], [5, 1], [3, 5]]\n",
        "\n",
        "X_multi = []\n",
        "y_multi = []\n",
        "\n",
        "for i, center in enumerate(centers):\n",
        "    X_class = np.random.randn(n_samples, 2) * 0.8 + np.array(center)\n",
        "    X_multi.append(X_class)\n",
        "    y_multi.extend([i] * n_samples)\n",
        "\n",
        "X_multi = np.vstack(X_multi)\n",
        "y_multi = np.array(y_multi)\n",
        "\n",
        "# Shuffle\n",
        "shuffle_idx = np.random.permutation(len(y_multi))\n",
        "X_multi = X_multi[shuffle_idx]\n",
        "y_multi = y_multi[shuffle_idx]\n",
        "\n",
        "# Split\n",
        "split = int(0.8 * len(y_multi))\n",
        "X_train_m, X_test_m = X_multi[:split], X_multi[split:]\n",
        "y_train_m, y_test_m = y_multi[:split], y_multi[split:]\n",
        "\n",
        "# Fit classifier\n",
        "clf_multi = ClassificationTree(max_depth=6)\n",
        "clf_multi.fit(X_train_m, y_train_m)\n",
        "\n",
        "print(\"Multiclass Classification (3 Classes)\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Training samples: {len(X_train_m)}\")\n",
        "print(f\"Test samples: {len(X_test_m)}\")\n",
        "print(f\"Number of classes: {clf_multi.n_classes_}\")\n",
        "print(f\"Training accuracy: {clf_multi.score(X_train_m, y_train_m):.2%}\")\n",
        "print(f\"Test accuracy: {clf_multi.score(X_test_m, y_test_m):.2%}\")\n",
        "print(f\"Tree depth: {clf_multi.get_depth()}\")\n",
        "print(f\"Number of leaves: {clf_multi.get_n_leaves()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize multiclass classification\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "plot_decision_boundary(clf_multi, X_train_m, y_train_m, \n",
        "                      title=\"Multiclass - Training Data\", ax=axes[0])\n",
        "plot_decision_boundary(clf_multi, X_test_m, y_test_m, \n",
        "                      title=\"Multiclass - Test Data\", ax=axes[1],\n",
        "                      show_misclassified=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Hyperparameter Tuning Analysis\n",
        "\n",
        "Analyze how `max_depth` affects model performance and complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate complex dataset for hyperparameter analysis\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 300\n",
        "X_complex = np.random.randn(n_samples, 2) * 2\n",
        "# Create circular decision boundary\n",
        "y_complex = ((X_complex[:, 0] ** 2 + X_complex[:, 1] ** 2) > 2).astype(int)\n",
        "# Add label noise (5%)\n",
        "noise_idx = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
        "y_complex[noise_idx] = 1 - y_complex[noise_idx]\n",
        "\n",
        "# Split\n",
        "split = int(0.7 * n_samples)\n",
        "X_train_c, X_test_c = X_complex[:split], X_complex[split:]\n",
        "y_train_c, y_test_c = y_complex[:split], y_complex[split:]\n",
        "\n",
        "# Test different max_depth values\n",
        "depths = range(1, 15)\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "n_leaves_list = []\n",
        "\n",
        "for depth in depths:\n",
        "    clf_d = ClassificationTree(max_depth=depth)\n",
        "    clf_d.fit(X_train_c, y_train_c)\n",
        "    \n",
        "    train_accs.append(clf_d.score(X_train_c, y_train_c))\n",
        "    test_accs.append(clf_d.score(X_test_c, y_test_c))\n",
        "    n_leaves_list.append(clf_d.get_n_leaves())\n",
        "\n",
        "print(\"Hyperparameter Analysis Complete!\")\n",
        "print(f\"Dataset: Circular decision boundary with 5% label noise\")\n",
        "print(f\"Training samples: {len(X_train_c)}, Test samples: {len(X_test_c)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot hyperparameter analysis\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy vs Depth\n",
        "axes[0].plot(list(depths), train_accs, 'o-', color='#3498DB', linewidth=2, \n",
        "             markersize=8, label='Training Accuracy')\n",
        "axes[0].plot(list(depths), test_accs, 's-', color='#E74C3C', linewidth=2, \n",
        "             markersize=8, label='Test Accuracy')\n",
        "axes[0].fill_between(list(depths), train_accs, test_accs, alpha=0.2, color='gray',\n",
        "                     label='Generalization Gap')\n",
        "axes[0].set_xlabel('Max Depth', fontsize=12)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_title('Accuracy vs Tree Depth', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].set_ylim(0.5, 1.05)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Complexity vs Depth\n",
        "axes[1].plot(list(depths), n_leaves_list, 'o-', color='#27AE60', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Max Depth', fontsize=12)\n",
        "axes[1].set_ylabel('Number of Leaves', fontsize=12)\n",
        "axes[1].set_title('Tree Complexity vs Depth', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best depth\n",
        "best_idx = np.argmax(test_accs)\n",
        "print(f\"\\nBest max_depth: {list(depths)[best_idx]}\")\n",
        "print(f\"  Training accuracy: {train_accs[best_idx]:.2%}\")\n",
        "print(f\"  Test accuracy: {test_accs[best_idx]:.2%}\")\n",
        "print(f\"  Number of leaves: {n_leaves_list[best_idx]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize decision boundaries at different depths\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "selected_depths = [1, 2, 3, 5, 8, 12]\n",
        "\n",
        "for ax, depth in zip(axes, selected_depths):\n",
        "    clf_viz = ClassificationTree(max_depth=depth)\n",
        "    clf_viz.fit(X_train_c, y_train_c)\n",
        "    \n",
        "    plot_decision_boundary(clf_viz, X_train_c, y_train_c, ax=ax,\n",
        "                          title=f\"max_depth={depth} (leaves={clf_viz.get_n_leaves()})\")\n",
        "    \n",
        "    # Add accuracy annotation\n",
        "    test_acc = clf_viz.score(X_test_c, y_test_c)\n",
        "    train_acc = clf_viz.score(X_train_c, y_train_c)\n",
        "    ax.annotate(f'Train: {train_acc:.1%}\\nTest: {test_acc:.1%}', \n",
        "               xy=(0.02, 0.98), xycoords='axes fraction',\n",
        "               fontsize=10, ha='left', va='top',\n",
        "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.suptitle('Decision Boundaries at Different Tree Depths', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Overfitting Demonstration\n",
        "\n",
        "Demonstrating how deep trees can overfit to training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare underfitting vs good fit vs overfitting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "configs = [\n",
        "    (2, \"Underfitting (depth=2)\"),\n",
        "    (5, \"Good Fit (depth=5)\"),\n",
        "    (15, \"Overfitting (depth=15)\")\n",
        "]\n",
        "\n",
        "for ax, (depth, title) in zip(axes, configs):\n",
        "    clf_of = ClassificationTree(max_depth=depth)\n",
        "    clf_of.fit(X_train_c, y_train_c)\n",
        "    \n",
        "    plot_decision_boundary(clf_of, X_test_c, y_test_c, title=title, ax=ax,\n",
        "                          show_misclassified=True)\n",
        "    \n",
        "    train_acc = clf_of.score(X_train_c, y_train_c)\n",
        "    test_acc = clf_of.score(X_test_c, y_test_c)\n",
        "    gap = train_acc - test_acc\n",
        "    \n",
        "    ax.annotate(f'Train: {train_acc:.1%}\\nTest: {test_acc:.1%}\\nGap: {gap:.1%}', \n",
        "               xy=(0.02, 0.98), xycoords='axes fraction',\n",
        "               fontsize=11, ha='left', va='top',\n",
        "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
        "\n",
        "plt.suptitle('Bias-Variance Tradeoff: Underfitting vs Overfitting', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated the **iterative** implementation of a Classification Tree (CART):\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Basic Classification** - The iterative CART correctly classifies linearly separable data with 100% accuracy\n",
        "\n",
        "2. **Non-linear Boundaries** - The tree can learn complex decision boundaries (XOR problem) through multiple axis-aligned splits\n",
        "\n",
        "3. **Multiclass Support** - Works seamlessly with 3+ classes\n",
        "\n",
        "4. **Hyperparameter Tuning**:\n",
        "   - `max_depth` controls the bias-variance tradeoff\n",
        "   - Shallow trees → underfitting (high bias)\n",
        "   - Deep trees → overfitting (high variance)\n",
        "   - Optimal depth balances training and test accuracy\n",
        "\n",
        "5. **Overfitting Prevention**:\n",
        "   - Use `max_depth` to limit tree complexity\n",
        "   - Use `min_samples_split` and `min_samples_leaf` for additional regularization\n",
        "\n",
        "### Implementation Highlights:\n",
        "- Uses **stack-based iteration** instead of recursion\n",
        "- **Gini impurity** as the splitting criterion\n",
        "- Supports probability predictions via `predict_proba()`\n",
        "- Provides tree statistics: `get_depth()`, `get_n_leaves()`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
